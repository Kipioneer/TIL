{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오류가 있어서 확인하고 진행한다고 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 꽃 이름\n",
    "\n",
    "CLASS_NAMES = ['daisy','dandelion','roses','sunflowers', 'tulips']\n",
    "CLASS_NAMES\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "#이미지 전처리\n",
    "\n",
    "def read_and_decode(filename, reshape_dims):\n",
    "  img = tf.io.read_file(filename)\n",
    "  # 3차원 uint8 텐서로 변환\n",
    "  img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "  # 0.0 ~ 1.0 정규화\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  return tf.image.resize(img, reshape_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def decode_csv(csv_row):\n",
    "  record_defaults = [\"path\", \"flower\"]\n",
    "  filename, label_string = tf.io.decode_csv(csv_row, record_defaults)\n",
    "  img = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n",
    "  label = tf.argmax(tf.math.equal(CLASS_NAMES, label_string))\n",
    "  return img, label\n",
    "\n",
    "train_dataset = (tf.data.TextLineDataset(\"d:/data/flowers/train_set.csv\").map(decode_csv)).take(800).batch(BATCH_SIZE)\n",
    "eval_dataset = (tf.data.TextLineDataset(\"d:/data/flowers/eval_set.csv\").map(decode_csv)).take(200).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 튜닝\n",
    "# pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "def build_model(hp):\n",
    "  lrate = hp.Float('lrate', 1e-4, 1e-1, sampling='log')\n",
    "  l2 = hp.Choice('l2', values=[1e-1, 1e-2, 1e-3, 1e-4])\n",
    "  num_hidden = hp.Int('num_hidden', 8, 16, 32, 64, 128)\n",
    "  regularizer = tf.keras.regularizers.l2(l2)\n",
    "  model = tf.keras.Sequential([\n",
    "              tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "              tf.keras.layers.Dense(num_hidden,\n",
    "                                    kernel_regularizer=regularizer,\n",
    "                                    activation=tf.keras.activations.relu),\n",
    "\n",
    "              tf.keras.layers.Dense(len(CLASS_NAMES),\n",
    "                                    kernel_regularizer=regularizer,\n",
    "                                    activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lrate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률적인 방법으로 최적의 하이퍼파라미터를 계산하는 방식\n",
    "# GridSearch : 모든 조합을 테스트하는 방식\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#     build_model,\n",
    "#     objective=kt.Objective('val_accuracy', 'max'),max_trials=10)  \n",
    "# tuner.search(\n",
    "#     train_dataset, validation_data=eval_dataset, epochs=20,callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 안됨\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',  # 목표 메트릭을 'val_accuracy'로 설정\n",
    "    max_trials=10\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    validation_data=eval_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(tuner\u001b[39m.\u001b[39mget_best_hyperparameters(\u001b[39m2\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(tuner\u001b[39m.\u001b[39mget_best_hyperparameters(\u001b[39m2\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mvalues)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "print(tuner.get_best_hyperparameters(2)[0].values)\n",
    "print(tuner.get_best_hyperparameters(2)[1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "lrate = 0.0001\n",
    "l2 = 0\n",
    "dropout_prob = 0.2\n",
    "num_hidden = [32, 16]\n",
    "\n",
    "# layer의 파라미터ㅡ 출력값에 대한 패널티 설정\n",
    "# l1: Manhatten distance\n",
    "# l2: Euclidean distance\n",
    "\n",
    "#regularizer = tf.keras.reularizers.l1_l2(l1,l2)\n",
    "regularizer = tf.keras.regularizers.l2(l2)\n",
    "layers = [tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid keyword argument(s) in `compile()`: ({'mtrics'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m layers\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     10\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39mlen\u001b[39m(CLASS_NAMES), kernel_regularizer\u001b[39m=\u001b[39mregularizer, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     11\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential(layers)\n\u001b[1;32m---> 12\u001b[0m model\u001b[39m.\u001b[39;49mcompile(optimizer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(learning_rate\u001b[39m=\u001b[39;49mlrate),\n\u001b[0;32m     13\u001b[0m                 loss\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mSparseCategoricalCrossentropy(),\n\u001b[0;32m     14\u001b[0m                 mtrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3787\u001b[0m, in \u001b[0;36mModel._validate_compile\u001b[1;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[0;32m   3785\u001b[0m invalid_kwargs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(kwargs) \u001b[39m-\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39msample_weight_mode\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m   3786\u001b[0m \u001b[39mif\u001b[39;00m invalid_kwargs:\n\u001b[1;32m-> 3787\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   3788\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid keyword argument(s) in `compile()`: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3789\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m(invalid_kwargs,)\u001b[39m}\u001b[39;00m\u001b[39m. Valid keyword arguments include \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3790\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcloning\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexperimental_run_tf_function\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdistribute\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   3791\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtarget_tensors\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39msample_weight_mode\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   3792\u001b[0m     )\n\u001b[0;32m   3794\u001b[0m \u001b[39m# Model must be created and compiled with the same DistStrat.\u001b[39;00m\n\u001b[0;32m   3795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39mand\u001b[39;00m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mhas_strategy():\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile()`: ({'mtrics'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\"."
     ]
    }
   ],
   "source": [
    "for hno, nodes in enumerate(num_hidden):\n",
    "    layers.extend([\n",
    "        tf.keras.layers.Dense(nodes, kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(rate=dropout_prob),\n",
    "    ])\n",
    "\n",
    "layers.append(\n",
    "    tf.keras.layers.Dense(len(CLASS_NAMES), kernel_regularizer=regularizer, activation='softmax'))\n",
    "model = tf.keras.Sequential(layers)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lrate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                mtrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
