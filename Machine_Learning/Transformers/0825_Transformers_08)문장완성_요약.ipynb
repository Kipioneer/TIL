{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-08-24T16:51:22.031547Z",
     "end_time": "2023-08-24T16:51:22.047502Z"
    }
   },
   "outputs": [],
   "source": [
    "# BigBird 모형: 더 긴 입력 시퀀스를 처리할 수 있는 모형\n",
    "# 2048 토큰 처리(BERT의 4배, 512x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "BigBirdForMaskedLM(\n  (bert): BigBirdModel(\n    (embeddings): BigBirdEmbeddings(\n      (word_embeddings): Embedding(50358, 768, padding_idx=0)\n      (position_embeddings): Embedding(4096, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BigBirdEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BigBirdLayer(\n          (attention): BigBirdAttention(\n            (self): BigBirdBlockSparseAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): BigBirdSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BigBirdIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): NewGELUActivation()\n          )\n          (output): BigBirdOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n  (cls): BigBirdOnlyMLMHead(\n    (predictions): BigBirdLMPredictionHead(\n      (transform): BigBirdPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): NewGELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=50358, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BigBirdTokenizer, BigBirdForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "model = BigBirdForMaskedLM.from_pretrained('google/bigbird-roberta-base')\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:43:13.913505Z",
     "end_time": "2023-08-25T09:43:29.140297Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(decoder): Linear(in_features=768, out_features=50358, bias=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "inputs = ['I like reading [MASK].',\n",
    "          'I like driving a [MASK]',\n",
    "          'The world is facing with a [MASK] [MASK] crisis. We are all suffering from infectious diseases.',]\n",
    "answers = ['I like reading book.',\n",
    "           'I like driving a car',\n",
    "           'The world is facing with a pandemic crisis. We are all suffering from infectious diseases.']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:43:30.840957Z",
     "end_time": "2023-08-25T09:43:30.878855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "encoded_inputs = []\n",
    "encoded_labels =  []\n",
    "\n",
    "for i, l in zip(inputs, answers):\n",
    "  encoded_inputs.append(tokenizer(i, return_tensors=\"pt\"))\n",
    "  #                             입력\n",
    "  encoded_labels.append(tokenizer(l, return_tensors=\"pt\")[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:43:32.343918Z",
     "end_time": "2023-08-25T09:43:32.369849Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 7 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss：11.183554649353027\n",
      "prediction：i like reading it . i\n",
      "answer：I like reading book.\n",
      "\n",
      "\n",
      "loss：8.141962051391602\n",
      "prediction：much like driving a car much\n",
      "answer：I like driving a car\n",
      "\n",
      "\n",
      "loss：4.29605770111084\n",
      "prediction：the world is facing with a global health crisis . we are all suffering from infectious diseases . .\n",
      "answer：The world is facing with a pandemic crisis. We are all suffering from infectious diseases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 추론 모드로 실행\n",
    "#추론 모드로 실행\n",
    "\n",
    "for input, label in zip(encoded_inputs, encoded_labels):\n",
    "\n",
    "  outputs = model(**input, labels=label)\n",
    "  loss = outputs.loss\n",
    "  logits = outputs.logits\n",
    "  #          final hidden\n",
    "\n",
    "  print(f\"loss：{loss.item()}\")\n",
    "  print(f\"prediction：{' '.join([tokenizer.decode(logits[0][i].argmax(-1)) for i in range(1, len(logits[0]))])}\")\n",
    "  print(f\"answer：{tokenizer.decode(label[0][1:-1])}\")\n",
    "  print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:43:33.196521Z",
     "end_time": "2023-08-25T09:43:36.937597Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# pegasus: 문장 요약에 특화된 사전 학습 모형, 구글 2020 발표\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "# 모델명\n",
    "model_name = 'google/pegasus-xsum'\n",
    "device = 'cpu'\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:45:53.570276Z",
     "end_time": "2023-08-25T09:46:14.113180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "inputs = [\"\"\"\n",
    "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.\n",
    "\"\"\"]\n",
    "\n",
    "batch = tokenizer(inputs, truncation=True, padding='longest', return_tensors='pt').to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:46:18.921780Z",
     "end_time": "2023-08-25T09:46:18.962669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 3414, 18006,   423, 14849,  1261,  1581,   108,   253,   130,   110,\n         62613,   108,   148,  1358,   112,  2745,  6602,   124,   223,   710,\n          1261,  2196,   143, 72237,   158,  2722,   107,   611,   108,   205,\n          1133, 18006,  1645,   777,   124,   956,  2641,   110, 88758,   108,\n           253,   130,   990, 12967,   111,  1892,   107,   202, 19552, 13183,\n           117,   120,   254,  2641,   121,  7115,  1133, 18006,   137,  1280,\n           141,  1215,   135,   956,   121, 23802,  1261,  1581,   107, 13618,\n           201,   939,   120,   118,  9982,   122,  9878,  1596, 53541,  1352,\n           108,   253,   130,  5293, 25255,   108,  1133, 18006,  1261,  1581,\n           135,  5932,   602,   115,  4844,  6602,   204, 18266,  1133, 18006,\n           113,   956,   121, 23802,  1261,  1581,   107,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:46:22.007571Z",
     "end_time": "2023-08-25T09:46:22.047344Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining large neural language models can lead to substantial gains over continual pretraining of general-domain language models.\n"
     ]
    }
   ],
   "source": [
    "# 요약 문장 생성\n",
    "\n",
    "translated = model.generate(**batch)\n",
    "generated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(generated_text[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:46:33.585638Z",
     "end_time": "2023-08-25T09:46:42.717655Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "PegasusForConditionalGeneration(\n  (model): PegasusModel(\n    (shared): Embedding(96103, 1024, padding_idx=0)\n    (encoder): PegasusEncoder(\n      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n      (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n      (layers): ModuleList(\n        (0-15): 16 x PegasusEncoderLayer(\n          (self_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): PegasusDecoder(\n      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n      (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n      (layers): ModuleList(\n        (0-15): 16 x PegasusDecoderLayer(\n          (self_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-25T09:47:08.666223Z",
     "end_time": "2023-08-25T09:47:09.223050Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
