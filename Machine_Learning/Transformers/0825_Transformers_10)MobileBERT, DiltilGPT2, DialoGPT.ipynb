{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-08-24T17:55:17.012911Z",
     "end_time": "2023-08-24T17:55:17.025876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mobile BERT: BERT를 압축하고 속도를 개선한 모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)solve/main/vocab.txt', max=231508.0, style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24a5f5d58b6140d4b13a1002373ef0b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tjoeun\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)lve/main/config.json', max=847.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9ffbebb67224b82bd50065a642e4e48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading pytorch_model.bin', max=146863759.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9a44542593944e49251e129531e608c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "MobileBertModel(\n  (embeddings): MobileBertEmbeddings(\n    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 512)\n    (token_type_embeddings): Embedding(2, 512)\n    (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n    (LayerNorm): NoNorm()\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (encoder): MobileBertEncoder(\n    (layer): ModuleList(\n      (0-23): 24 x MobileBertLayer(\n        (attention): MobileBertAttention(\n          (self): MobileBertSelfAttention(\n            (query): Linear(in_features=128, out_features=128, bias=True)\n            (key): Linear(in_features=128, out_features=128, bias=True)\n            (value): Linear(in_features=512, out_features=128, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): MobileBertSelfOutput(\n            (dense): Linear(in_features=128, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n          )\n        )\n        (intermediate): MobileBertIntermediate(\n          (dense): Linear(in_features=128, out_features=512, bias=True)\n          (intermediate_act_fn): ReLU()\n        )\n        (output): MobileBertOutput(\n          (dense): Linear(in_features=512, out_features=128, bias=True)\n          (LayerNorm): NoNorm()\n          (bottleneck): OutputBottleneck(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (LayerNorm): NoNorm()\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (bottleneck): Bottleneck(\n          (input): BottleneckLayer(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n          )\n          (attention): BottleneckLayer(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n          )\n        )\n        (ffn): ModuleList(\n          (0-2): 3 x FFNLayer(\n            (intermediate): MobileBertIntermediate(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (intermediate_act_fn): ReLU()\n            )\n            (output): FFNOutput(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n        )\n      )\n    )\n  )\n  (pooler): MobileBertPooler()\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MobileBertTokenizer, MobileBertModel\n",
    "import torch\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer_mbert = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "model_mbert = MobileBertModel.from_pretrained('google/mobilebert-uncased')\n",
    "model_mbert"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T17:56:33.023916Z",
     "end_time": "2023-08-24T17:56:40.670543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model_bert"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T17:58:17.505713Z",
     "end_time": "2023-08-24T17:58:18.968309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobile', 'bert', 'is', 'more', 'practical', 'than', 'bert', '.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"Mobile bert is more practical than bert.\"\n",
    "\n",
    "# Mobile BERT 토크나이징\n",
    "inputs = tokenizer_mbert.tokenize(text)\n",
    "print(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T17:59:29.258917Z",
     "end_time": "2023-08-24T17:59:29.332932Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobile', 'bert', 'is', 'more', 'practical', 'than', 'bert', '.']\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이징\n",
    "inputs = tokenizer_bert.tokenize(text)\n",
    "print(inputs)\n",
    "# 두 모델의 실행결과가 같음"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:00:05.224853Z",
     "end_time": "2023-08-24T18:00:05.237853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = 'Mobile bert is more practical than bert.'\n",
    "inputs = tokenizer_mbert.encode(text)\n",
    "# squeeze() 사이즈가 1인 차원 제거, unsqeeeze() 첫번쨰 위치에 1인 차우너 추가\n",
    "outputs = model_mbert(torch.tensor(inputs).unsqueeze(0))\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "inputs = tokenizer_bert.encode(text)\n",
    "outputs = model_bert(torch.tensor(inputs).unsqueeze(0))\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# torch.Size([1, 10, 512])  BERT 모형\n",
    "# torch.Size([1, 10, 768])  Mobile BERT 모형"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:03:02.058926Z",
     "end_time": "2023-08-24T18:03:02.185587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of korea is seoul .\n"
     ]
    }
   ],
   "source": [
    "# Mobile BERT 추론\n",
    "from transformers import MobileBertTokenizer, MobileBertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "model = MobileBertForMaskedLM.from_pretrained('google/mobilebert-uncased')\n",
    "\n",
    "# 마스크한 문장 및 정답 문장을 각기 토크나이징\n",
    "inputs = tokenizer(\"The capital of Korea is [MASK].\", return_tensors='pt')\n",
    "labels = tokenizer(\"The capital of Korea is Seoul.\", return_tensors='pt')['input_ids']\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(' '.join([tokenizer.decode(i.item()).replace(\" \", \"\") for i in logits.argmax(-1)[0]][1:-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:07:10.642531Z",
     "end_time": "2023-08-24T18:07:16.481160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of japan is tokyo .\n"
     ]
    }
   ],
   "source": [
    "# BERT 추론\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 마스크한 문장 및 정답 문장을 각기 토크나이징\n",
    "inputs = tokenizer(\"The capital of Japan is [MASK].\", return_tensors='pt')\n",
    "labels = tokenizer(\"The capital of Japan is Tokyo.\", return_tensors='pt')['input_ids']\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(' '.join([tokenizer.decode(i.item()).replace(\" \", \"\") for i in logits.argmax(-1)[0]][1:-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:09:29.823044Z",
     "end_time": "2023-08-24T18:09:31.701653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)lve/main/config.json', max=762.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23b3b961274943f1a0992eb2b637429f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)olve/main/vocab.json', max=1042301.0, styl…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe8438f2d53d426793a1d4b12bd93221"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)olve/main/merges.txt', max=456318.0, style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f636b6adfad84abab03d06834074035d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)/main/tokenizer.json', max=1355256.0, styl…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f93025b9ea244877ab7b51a82ee56b7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading model.safetensors', max=352824413.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f030df258904056808ee9885a3748b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)neration_config.json', max=124.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93af3722aeab45148ab13227c2951b57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelWithLMHead.from_pretrained('distilgpt2')\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:10:50.858057Z",
     "end_time": "2023-08-24T18:11:00.718990Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good thing to have a good friend\n"
     ]
    }
   ],
   "source": [
    "# 문장완성\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "\n",
    "# greedy_output = model.generate(input_ids, max_length=12)\n",
    "greedy_output = model.generate(input_ids, max_length=15)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:14:05.087979Z",
     "end_time": "2023-08-24T18:14:05.390169Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)okenizer_config.json', max=26.0, style=Pro…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8de863e5366e485cb8580c0fcca28fb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)lve/main/config.json', max=641.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "531ef872f9df4378a012d8f15109cb44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)olve/main/vocab.json', max=1042301.0, styl…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8976e6e56ca746f09b3d5d6a3af94152"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)olve/main/merges.txt', max=456318.0, style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3730d2f33a5f470d8cc7284f851c136f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading model.safetensors', max=351256598.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c0e8ef568b44dd19c5ee6bebe3b56b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)neration_config.json', max=124.0, style=Pr…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c8462f0f42c4aefa542240e563bbc32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# 토크나이저 및 모델 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('microsoft/DialoGPT-small')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:15:13.667491Z",
     "end_time": "2023-08-24T18:15:23.802139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good way to get a feel for the game.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=30)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-24T18:17:14.801821Z",
     "end_time": "2023-08-24T18:17:15.460058Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
